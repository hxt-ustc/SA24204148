---
title: "Homework-2024.09.23"
author: "By SA24204148"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework-2024.09.23}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Question 5.4

Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf, and use the function to estimate $F(x)$ for $x = 0.1, 0.2,\cdots, 0.9$. Compare the estimates with the values returned by the **pbeta** function in R.

## Answer

To estimate of the Beta(3, 3) cdf,

 + Generate $X_1,\cdots,X_m$, iid from Uniform(0, x).
 + Compute $\bar{g}(X) = \frac{1}{m}\sum g(Xi)$.
 + $\hat{F}(x)=x\bar{g}(X)$

where g(x) is the density function of Beta(3, 3), i.e., $g(x)=30x^2(1-x)^2$ 

```{r}
beta33_cdf <- function(x, n =1000, seed=0){
  set.seed(seed)
  u = runif(n, max = x)
  return(x*mean(30*u^2*(1-u)^2))
}
```

```{r}
t = seq(0.1,0.9,0.1)
cdf.hat = sapply(t,beta33_cdf)
true.cdf = pbeta(t,3,3)
result = rbind(cdf.hat,true.cdf)
colnames(result) = t
result = rbind(result,cdf.hat-true.cdf)
result
```

From the table above, the estimates of $F(x)$ for $x = 0.1, 0.2,\cdots, 0.9$ are quite close to the values returned by the pbeta function in R.

## Question 5.9

The Rayleigh density is

$$
f(x)=\frac{x}{\sigma^2}e^{-x^2/(2\sigma^2)},\ \ \ \ \ x\ge0,\sigma>0.
$$
Implement a function to generate samples from a Rayleigh($\sigma$) distribution, using antithetic variables. What is the percent reduction in variance of $\frac{X+X'}{2}$ compared with $\frac{X_1+X_2}{2}$ for independent $X_1, X_2$?

## Answer

To generate samples from a Rayleigh($\sigma$) distribution, using antithetic variables:

 + Generate $U_1,\cdots,U_m$ iid from Uniform(0, 1).
 + Generate $X_i = F^{-1}(U_i)$, where $F^{-1}$ is the inverse distribution function of Rayleigh($\sigma$)
 + Generate $X_i' = F^{-1}(1-U_i)$
 + $\hat{\theta}=\frac{\sum(X_i+X_i')}{2m}$
 
Here $F_X(x)=1-e^{-x^2/(2\sigma^2)}$, for $0\leq x$,and $F_X^{-1}(u)=\sqrt{-2\sigma^2\ln(1-u)}$.
 
```{r}
rRayleigh <- function(sigma,n=1200,seed=0){
  set.seed(seed)
  u = runif(n)
  return(list(x1 = sqrt(-2*sigma^2*log(1-u)), x2 = sqrt(-2*sigma^2*log(u))))
}
```

The key point is the variance of $\hat{\theta}$ and $\tilde{\theta}=\frac{\sum X_{i1}+X_{i2}}{2m}$ where $X_{i1}$ and $X_{i2}$ are independent.

```{r}
sigma = c(1,2,5,10,20)
f1 <- function(sigma,n=1200,seed=0){
  x1 = rRayleigh(sigma,n,seed)
  x1 = (x1$x1 + x1$x2)/2
  x2 = (rRayleigh(sigma,n,seed)$x1 + rRayleigh(sigma,n,2*seed)$x1)/2
  return(var(x1)/var(x2))
}
round(1-sapply(sigma,f1),3)
```

For different $\sigma$, the proportion of variance reduction is 97.4%. Since the samples generated by antithetic variables are negatively correlated with each other, the variance is reduced by a certain percentage. As for the irrelevance of the variance reduction ratio to $\sigma$, it can be explained by the following formulas:

$Var(\hat{\theta})=\frac{1}{m}Var(\frac{X+X'}{2}) = \frac{1}{m}Var(\frac{\sqrt{-2\sigma^2\ln(1-U)}+\sqrt{-2\sigma^2\ln(U)}}{2})=\frac{2\sigma^2}{m}Var(\frac{\sqrt{-\ln(1-U)}+\sqrt{-\ln(U)}}{2})$

$Var(\tilde{\theta})=\frac{1}{m}Var(\frac{X_1+X_2}{2}) =\frac{1}{m}Var(\frac{\sqrt{-2\sigma^2\ln(1-U_1)}+\sqrt{-2\sigma^2\ln(1-U_2)}}{2})=\frac{2\sigma^2}{m}Var(\frac{\sqrt{-\ln(1-U_1)}+\sqrt{-\ln(1-U_2)}}{2})$

$\Rightarrow \frac{Var(\hat{\theta})}{Var(\tilde{\theta})}=\frac{Var(\sqrt{-\ln(1-U)}+\sqrt{-\ln(U)})}{Var(\sqrt{-\ln(1-U_1)}+\sqrt{-\ln(1-U_2)})}$ is independent with $\sigma$ .

```{r}
par(mfrow = c(1, 2))  # 设置图形区域为1行2列

sigma = 1

X_plus_independent = (rRayleigh(sigma, seed = 1)$x1 + rRayleigh(sigma,seed = 2)$x1)/2
X_plus_antithetic = rRayleigh(sigma)
X_plus_antithetic = (X_plus_antithetic$x1 + X_plus_antithetic$x2)/2
    
hist(X_plus_independent, breaks = 30, col = 'deepskyblue', 
         main = '独立变量生成的Rayleigh样本', 
         xlab = '样本值', 
         ylab = '频率', 
         xlim = c(0, max(X_plus_independent, X_plus_antithetic)),
         probability = TRUE)
    
hist(X_plus_antithetic, breaks = 30, col = 'deeppink', 
         main = '对立变量生成的Rayleigh样本', 
         xlab = '样本值', 
         ylab = '频率', 
         xlim = c(0, max(X_plus_independent, X_plus_antithetic)),
         probability = TRUE)
    
legend("topright", legend = c("独立变量", "对立变量"), 
           fill = c("deepskyblue", "deeppink"))
```


## Question 5.13

Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are ‘close’ to

$$
g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2},x>1
$$
Which of your two importance functions should produce the smaller variance in estimating

$$
\int_1^{\infty} \frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx
$$
by importance sampling? Explain.

## Answer

To estimate the above integral, the following two importance functions are used:

 + $f_1(x) = \sqrt{e}xe^{-x^2/2}, x>1$,
 + $f_2(x) = \frac{\phi(x)}{1-\Phi(1)}$,
 
where $\Phi$ and $\phi$ is cdf and pdf of the standard normal distribution respectively. In fact, the above two functions are truncated Rayleigh distribution and normal distribution, both of which can be easily sampled by the inverse distribution method. From the graph of the functions, the shape of $f_1$(red) is more similar to that of $g$(black), so it is guessed that the variance is also smaller.

```{r}
g <- function(x){
  return(x^2/sqrt(2*pi)*exp(-x^2/2))
}

f1 <- function(x){
  return(x*sqrt(exp(1))*exp(-(x)^2/2))
}

f2 <- function(x){
  return(dnorm(x)/(1-pnorm(1)))
}

curve(g,xlim = c(1,3),ylim=c(0,2),ylab = "")
curve(f1,add=T,col="red")
curve(f2,add=T,col="green")
legend("topright",legend = c("g","f1","f2"),col = c(1,"red","green"),lty=1)
```

```{r}
F1 <- function(x){
  return(sqrt(1-2*log(1-x)))
}

F2 <- function(x){
  return(qnorm(pnorm(1)+x*(1-pnorm(1))))
}

n = 1000
set.seed(0)
u = runif(n)
x1 = F1(u)
x2 = F2(u)
y1 = g(x1)/f1(x1)
y2 = g(x2)/f2(x2)
mean(y1)
mean(y2)
var(y1)
var(y2)
```

The integral estimates using the two importance functions are 0.402 and 0.405, respectively, which are very close. The variance of the former is smaller, the same as the previous projection.

## Question 

For $n=10^4,2\times10^4,4\times10^4,6\times10^4,8\times10^4$,apply the fast sorting algorithm to randomly permuted nummbers of $1,\cdots,n$.

Calculate computation time averaged over 100 simulations, denoted by $a_n$.

Regress $a_n$ on $t_n:=n\log(n)$, and graphically show the results(scatter plot and regression line).

## Answer

To apply the fast sorting algorithm:

 + Randomly pick one number, say $x_{(\xi)}$, and compare it with all other numbers, place the smaller ones to the left of $x_{(\xi)}$ and the larger ones to the right of $x_{(\xi)}$.
 + Apply last step to the left numbers and the right numbers separately. Repeat such procedure recursively until $x_1, \cdots, x_n$ are placed in an increasing order as $x_{(1)}, \cdots, x_{(n)}$.

```{r}
fast_sort <- function(x){
  if(length(x)<2) return(x)
  x0 <- sample(x,1)
  return(c(fast_sort(x[x<x0]),x0,fast_sort(x[x>x0])))
}
```

For different n, apply the fast sorting algorithm and record the duration of 150 simulations.

```{r}
n = c(1,2,4,6,8)*1e4
simulation = 150
times <- function(n,simulation){
  t = 0
  for(i in seq(simulation))  {
    x = sample(seq(n),n)
    start_time <- Sys.time()
    fast_sort(x)
    t = t + Sys.time()-start_time
    }
  return(t/simulation)
}

t = sapply(n,function(z) times(z,simulation))
```

Regress computation time $a_n$ on $t_n:=n\log(n)$. As can be seen from the image, the running time is linearly correlated with $t_n:=n\log(n)$

```{r}
y = n*log(n)
my.fit <- lm(t~y)
plot(n*log(n),t,ylab=expression(an))
curve(my.fit$coefficients[1]+my.fit$coefficients[2]*x,add = T)
```
